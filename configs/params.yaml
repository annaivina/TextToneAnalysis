#Training and main parameters which are applicable for many models:
batch_size: 16 #16 for the deberta  and 128 for the rest (256 could be used for transformer)
start_lr: 2e-5 #1e-6 for transformer and 1e-4 for the LSTM model; deberta 2e-5
weight_decay: 0.01 #1e-5 for LSTM and weight_decay=2e-4 for transformer ; deberta 0.01
target_lr: 3e-5 #3e-5 only for transformer 
alpha: 0.1 
epochs: 10
resume_from_checkpoint: False
checkpoint_path: None #or any other name shall be pared with resume_from_checkpoint: True

#For the Early stopping callback:
callback:
  patience: 3
  monitor: val_loss
  mode: min
  path: best_model.pth